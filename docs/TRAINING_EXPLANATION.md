# AIの学習方法について

## 対戦相手は誰か？

このAIの学習では**セルフプレイ（自己対戦）**を使用しています。

### セルフプレイとは

- AIが**自分自身と対戦**して学習します
- プレイヤー1とプレイヤー2の両方を同じニューラルネットワークが操作
- 各手番で、その時点での盤面状態から最適な手を選択

### なぜセルフプレイが効果的か

1. **事前の対戦相手が不要**
   - 人間のプレイデータがなくても学習できる
   - 新しいゲームでも適用可能

2. **バランスの取れた学習**
   - 両プレイヤーが同じ強さなので、偏りのない学習
   - 一方的な勝利/敗北が減る

3. **継続的な改善**
   - ネットワークが強くなると、対戦相手（自分自身）も強くなる
   - 常に適切な難易度で学習が進む

### 学習の流れ

```
1. ゲーム開始
   ↓
2. Player 1の手番: ニューラルネットワークで最良の手を選択
   ↓
3. Player 2の手番: 同じニューラルネットワークで最良の手を選択
   ↓
4. ゲーム終了まで繰り返し
   ↓
5. 勝敗に基づいて報酬を割り当て
   - 勝利: +1.0
   - 引き分け: 0.0
   - 敗北: -1.0
   ↓
6. TD学習で価値関数を更新
   ↓
7. 次のゲームへ（ネットワークが少し賢くなる）
```

## 改善した入力情報（5ch → 12ch）

### 以前の問題点
元の実装では**5チャンネル**のみ:
- コマの位置(P1, P2)
- タイルの色(白、黒、グレー)

これでは**戦略的判断に必要な情報が不足**していました。

### 改善後の入力（12チャンネル）

| チャンネル | 内容 | 説明 |
|----------|------|------|
| 0 | プレイヤー1のコマ | コマの位置を1で表現 |
| 1 | プレイヤー2のコマ | コマの位置を1で表現 |
| 2 | 白タイル | 白タイルの位置 |
| 3 | 黒タイル | 黒タイルの位置 |
| 4 | グレータイル | グレータイルの位置 |
| **5** | **現在の手番** | **P1なら1.0、P2なら0.0（全マス同じ値）** |
| **6** | **P1のゴール距離** | **各コマからゴールまでの正規化距離** |
| **7** | **P2のゴール距離** | **各コマからゴールまでの正規化距離** |
| **8** | **P1の残り黒タイル** | **残数/5（全マス同じ値）** |
| **9** | **P1の残りグレータイル** | **残数/5（全マス同じ値）** |
| **10** | **P2の残り黒タイル** | **残数/5（全マス同じ値）** |
| **11** | **P2の残りグレータイル** | **残数/5（全マス同じ値）** |

### なぜこれらの情報が重要か

1. **手番情報（ch5）**
   - 攻撃か守備かの判断に必須
   - 同じ盤面でも手番によって評価が変わる

2. **ゴールまでの距離（ch6,7）**
   - 勝利条件が「相手陣地に到達」なので最重要
   - どのコマを優先的に進めるべきか判断できる

3. **残りタイル数（ch8-11）**
   - タイル配置のタイミング判断
   - 相手の残りリソースを考慮した戦略
   - 終盤での駆け引き

### ネットワークの改善

- **パラメータ数**: 約80万 → **約380万**（4.7倍）
- **畳み込み層**: 32→64→128ch → **64→128→256ch**
- **全結合層**: 3層 → **4層**（より深い推論）

## 学習パラメータの調整

```python
learning_rate: 0.001 → 0.0005  # より慎重な学習
epsilon: 0.1 → 0.15            # 探索をやや増やす
discount_factor: 0.99          # 長期的な報酬を重視
```

## ルール修正

### 修正前の誤り
- **相手のコマを取れる**（チェスのような取り合い）

### 正しいルール
- **相手のコマがいる場所には移動できない**
- コマの取り合いはなく、位置取りと到達が重要
- より戦略的な配置とタイル利用が求められる

この修正により、AIは「コマを取る」戦略から「ゴールへの最短ルートを確保する」戦略に学習が変わります。
